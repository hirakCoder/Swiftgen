#!/usr/bin/env python3
"""
Integration script to replace GPT-4/Claude with fine-tuned models in SwiftGen
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional

# Add backend to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from specialized_agents import AgentOrchestrator
from llama_finetuning_pipeline import CodeLlamaFineTuner

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FineTunedModelIntegration:
    """Integrates fine-tuned models into SwiftGen"""
    
    def __init__(self, model_path: str = "swift_codellama_model/final_model"):
        self.model_path = Path(model_path)
        self.model = None
        self.orchestrator = None
        
    def load_model(self):
        """Load the fine-tuned model"""
        logger.info(f"Loading fine-tuned model from {self.model_path}")
        
        if self.model_path.exists():
            # Load fine-tuned CodeLlama
            self.model = CodeLlamaFineTuner()
            self.model.model = None  # Will be loaded on first use
            self.model.output_dir = self.model_path.parent
            
            # Create orchestrator with model
            self.orchestrator = AgentOrchestrator(self.model)
            logger.info("Fine-tuned model loaded successfully")
            return True
        else:
            logger.warning(f"Model not found at {self.model_path}")
            logger.info("Using mock model for demonstration")
            self.orchestrator = AgentOrchestrator(None)
            return False
    
    def generate_app(self, request: Dict) -> Dict:
        """Generate app using fine-tuned model"""
        if not self.orchestrator:
            self.load_model()
        
        # Convert request to agent format
        agent_request = {
            "description": request.get("description", ""),
            "app_name": request.get("app_name", "MyApp"),
            "app_type": request.get("app_type", "generic")
        }
        
        # Process with orchestrator
        result = self.orchestrator.process_request(agent_request)
        
        # Convert back to SwiftGen format
        if result.get("success"):
            return {
                "status": "success",
                "files": result.get("files", []),
                "message": f"Generated by {result.get('agent', 'unknown')} agent"
            }
        else:
            return {
                "status": "error",
                "message": result.get("error", "Generation failed")
            }
    
    def modify_app(self, request: Dict) -> Dict:
        """Modify app using fine-tuned model"""
        if not self.orchestrator:
            self.load_model()
        
        # Convert request to agent format
        agent_request = {
            "modification": request.get("modification_request", ""),
            "files": request.get("files", []),
            "build_errors": request.get("build_errors", [])
        }
        
        # Process with orchestrator
        result = self.orchestrator.process_request(agent_request)
        
        # Convert back to SwiftGen format
        if result.get("success"):
            return {
                "status": "success",
                "files": result.get("files", []),
                "changes_made": result.get("changes_made", []),
                "message": f"Modified by {result.get('agent', 'unknown')} agent"
            }
        else:
            return {
                "status": "error",
                "message": result.get("error", "Modification failed")
            }
    
    def create_enhanced_claude_service_wrapper(self):
        """Create a wrapper that replaces enhanced_claude_service.py"""
        wrapper_code = '''#!/usr/bin/env python3
"""
Enhanced Claude Service - Now using fine-tuned models
"""

from integrate_finetuned_models import FineTunedModelIntegration
import asyncio

class EnhancedClaudeService:
    """Wrapper to maintain API compatibility"""
    
    def __init__(self):
        self.model_integration = FineTunedModelIntegration()
        self.model_integration.load_model()
    
    async def generate_ios_app_code(self, description: str, app_name: str, 
                                   app_type: str = "general", **kwargs) -> Dict:
        """Generate iOS app using fine-tuned model"""
        request = {
            "description": description,
            "app_name": app_name,
            "app_type": app_type
        }
        
        # Run synchronously (the fine-tuned model doesn't need async)
        result = self.model_integration.generate_app(request)
        
        # Convert to expected format
        if result["status"] == "success":
            return {
                "files": result["files"],
                "summary": result.get("message", "App generated successfully"),
                "complexity_score": 0.8,
                "estimated_tokens": 1000
            }
        else:
            raise Exception(result["message"])
    
    async def process_modification_request(self, modification_request: str, 
                                         files: List[Dict], **kwargs) -> Dict:
        """Process modification using fine-tuned model"""
        request = {
            "modification_request": modification_request,
            "files": files
        }
        
        result = self.model_integration.modify_app(request)
        
        if result["status"] == "success":
            return {
                "files": result["files"],
                "changes_made": result.get("changes_made", []),
                "summary": result.get("message", "Modification completed")
            }
        else:
            raise Exception(result["message"])


# Global instance
enhanced_service = EnhancedClaudeService()
'''
        
        # Save the wrapper
        wrapper_path = Path("enhanced_claude_service_finetuned.py")
        with open(wrapper_path, 'w') as f:
            f.write(wrapper_code)
        
        logger.info(f"Created wrapper at {wrapper_path}")
        return wrapper_path


def update_main_py():
    """Update main.py to use fine-tuned models"""
    logger.info("Updating main.py to use fine-tuned models...")
    
    # Read current main.py
    with open("main.py", 'r') as f:
        content = f.read()
    
    # Create backup
    backup_path = f"main_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py"
    with open(backup_path, 'w') as f:
        f.write(content)
    logger.info(f"Created backup: {backup_path}")
    
    # Replace import
    content = content.replace(
        "from enhanced_claude_service import EnhancedClaudeService",
        "from enhanced_claude_service_finetuned import EnhancedClaudeService"
    )
    
    # Save updated main.py
    with open("main.py", 'w') as f:
        f.write(content)
    
    logger.info("Updated main.py to use fine-tuned models")


def test_integration():
    """Test the integration"""
    logger.info("\nTesting fine-tuned model integration...")
    
    integration = FineTunedModelIntegration()
    integration.load_model()
    
    # Test app generation
    logger.info("\n1. Testing app generation...")
    gen_request = {
        "description": "Create a simple todo list app",
        "app_name": "TodoTest"
    }
    gen_result = integration.generate_app(gen_request)
    logger.info(f"Generation result: {gen_result['status']}")
    
    # Test modification
    logger.info("\n2. Testing modification...")
    if gen_result["status"] == "success":
        mod_request = {
            "modification_request": "Add dark mode toggle",
            "files": gen_result.get("files", [])
        }
        mod_result = integration.modify_app(mod_request)
        logger.info(f"Modification result: {mod_result['status']}")
    
    # Test debug/error fixing
    logger.info("\n3. Testing error fixing...")
    debug_request = {
        "modification_request": "Fix build errors",
        "files": gen_result.get("files", []),
        "build_errors": ["cannot find 'ErrorView' in scope"]
    }
    debug_result = integration.modify_app(debug_request)
    logger.info(f"Debug result: {debug_result['status']}")
    
    # Show agent statistics
    logger.info("\n4. Agent statistics:")
    stats = integration.orchestrator.get_agent_stats()
    for agent, data in stats.items():
        logger.info(f"  {agent}: {data['total']} requests, " +
                   f"{data['successful']} successful, " +
                   f"avg confidence: {data['avg_confidence']:.2f}")


if __name__ == "__main__":
    import argparse
    from datetime import datetime
    
    parser = argparse.ArgumentParser(description="Integrate fine-tuned models with SwiftGen")
    parser.add_argument("--create-wrapper", action="store_true",
                       help="Create enhanced_claude_service wrapper")
    parser.add_argument("--update-main", action="store_true",
                       help="Update main.py to use fine-tuned models")
    parser.add_argument("--test", action="store_true",
                       help="Test the integration")
    parser.add_argument("--full-integration", action="store_true",
                       help="Perform full integration (wrapper + update + test)")
    
    args = parser.parse_args()
    
    if args.full_integration:
        # Full integration process
        logger.info("Starting full integration process...")
        
        # 1. Create wrapper
        integration = FineTunedModelIntegration()
        wrapper_path = integration.create_enhanced_claude_service_wrapper()
        
        # 2. Update main.py
        update_main_py()
        
        # 3. Test
        test_integration()
        
        logger.info("\nIntegration complete!")
        logger.info("Fine-tuned models are now integrated with SwiftGen")
        
    else:
        if args.create_wrapper:
            integration = FineTunedModelIntegration()
            integration.create_enhanced_claude_service_wrapper()
        
        if args.update_main:
            update_main_py()
        
        if args.test:
            test_integration()
        
        if not any([args.create_wrapper, args.update_main, args.test]):
            parser.print_help()